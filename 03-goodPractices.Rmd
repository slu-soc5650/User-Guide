# "Good Enough" Research Practices

This section introduces some of the core concepts that we will emphasize in this course throughout the semester. The title takes inspiration from a recent article titled ["Good Enough Practices in Scientific Computing"](https://arxiv.org/abs/1609.00037)^[Wilson, G., Bryan, J., Cranston, K., Kitzes, J., Nederbragt, L. and Teal, T.K., 2016. Good Enough Practices in Scientific Computing. *arXiv preprint arXiv:1609.00037.*]. The authors note in their introduction that scientific computing advice can sometimes be both overwhelming and focused on tools that are inaccessible to many analysts. Their goal, and the goal of this course, is to de-mystify the simplist tools that enable researchers to streamline their workflows:

> Our intended audience is researchers who are working alone or with a handful of collaborators on projects lasting a few days to a few months, and who are ready to move beyond emailing themselves a spreadsheet named `results-updated-3-revised.xlsx` at the end of the workday...Many of our recommendations are for the benefit of the collaborator every researcher cares about most: their future self.

I would argue that the skills they describe are useful beyond just a few months. Indeed, most of the skills here can dramatically improve students' dissertation experiences:

> Most importantly, these practices make researchers more productive individually by enabling them to get more done in less time and with less pain. They also accelerate research as a whole by making computational work (which increasingly means all work) more reproducible. But progress will not happen by itself. Universities and funding agencies need to support training for researchers in the use of these tools. Such investment will improve confidence in the results of computational work and allow us to make more rapid progress on important research questions.

While much of what we will talk about in this course is aimed at supporting your work, there are benefits that extend beyond your dissertation or your research projects. These benefits, which include developing sustainable workflows and structuring the way you interact with your own computer, can make everyday computing practices like checking email or organizing files an easier, more structured process.

## Reproducibility

One of the mantras of this course is our emphasis on reproducibility. The unifying feature of all of the "good enough" research practices discussed below is that they contribute to a more reproducible research product. 

Reproducibility is very much in vogue right now for number of reasons. [Assessments of studies in psychology](http://science.sciencemag.org/content/349/6251/aac4716)^[Open Science Collaboration, 2015. Estimating the reproducibility of psychological science. *Science*, 349(6251), p.aac4716.], for example, have found weaker on average effect sizes and far fewer statistically significant results than the initial studies reported. There have also been high profile instances of falsified research, including [research by a graduate student at UCLA](http://nymag.com/scienceofus/2015/05/how-a-grad-student-uncovered-a-huge-fraud.html). This particular instance of fraud was identified by gradute students intent on replicating the original study. 

At the same time, there is a recognition that the skills necessary for producing reproducible research are not being fostered in academic disciplines and graduate programs. Thus one of the goals of this course, and this **User's Guide** in particular, is to help develop a working knowledge of many of these skills.

One challenge, however, is that reproducibility does not have a consistent definition. Some researchers use the term to narrowly refer to code that can execute without alteration on a person's computer. Others use it to refer to research designs that can be replicated by other researchers. Still others discuss reproducibility as the ability to obtain a similar set of results or draw similar inferences from identical research designs. 

When we talk about reproducibility in this class. We'll be primarily concerned with **methods reproducibility**:

> the ability to implement, as exactly as possible, the experimental and computational procedures, with the same data and tools, to obtain the same results.^[Goodman, S.N., Fanelli, D. and Ioannidis, J.P., 2016. What does research reproducibility mean?. *Science translational medicine*, 8(341), pp.341ps12-341ps12.]

Methods reproducibility in GISc means that other analysts have full access to both the original data and the steps used to render those original data into a final research product, such as a set of maps. This is increasingly seen not just a matter of good research methodology, but as a matter of research ethics as well. Being able to be transparent with research decreases the potential for cases like the [fraudulant dissertation research conducted by a UCLA graduate student named Michael LaCour](http://nymag.com/scienceofus/2015/05/how-a-grad-student-uncovered-a-huge-fraud.html). It was the efforts of [two Standford graduate students who wanted to reproduce LaCour's findings](https://fivethirtyeight.com/features/how-two-grad-students-uncovered-michael-lacour-fraud-and-a-way-to-change-opinions-on-transgender-rights/) that ultimately led to the identification of problematic work. 

For GISc, methods reproducibility is derived from a number of sources. The first source is the use of **computer code** for working with data. Rather than making manual changes to tabular data in a spreadsheet application like Microsoft Excel, computer code provides detailed records of each individual alterations. Code can be used execute tasks repeatedly, meaning that errors can be easily fixed if they are discovered an hour, a day, a week, or a month later. During this semester, we'll use Stata's programming language to execute reproducible data cleaning processes.

Operations in ArcGIS can also be scripted using the programming language [Python](https://www.python.org). Python is an open-source language that is widely used by data analysts and computer programmers. We will not learn [ArcPy](http://pro.arcgis.com/en/pro-app/arcpy/get-started/what-is-arcpy-.htm), the library of Python commands for ArcGIS, this semester. However, it is important to know that many of the things we will learn this semester *can* be scripted, dramatically increasing the reproducibility of your work.^[For those of you who are interested, we'll be providing Python/ArcPy examples for many of the ArcGIS tasks we learn this semester. These will be available on GitHub in the `ArcPy` repository for those of you interested in expanding your knowledge.]

Since we won't focus on scripting for ArcGIS this semester, much of the work we will do will be done manually. This means that no record exists of the changes we make or the steps that we take to complete a task. From a reproducibility standpoint, this is problematic. Even if we were scripting our work in ArcGIS, there are often aspects of projects that must be completed manually. In GISc, this often arises in initial steps like download data or in the production of final map products, which often require using graphic design software. 

The second source of reproducibility in GISc is therefore derived from the **documentation** that we create to accompany our research products. These documents outline where our data originated (GIS metadata files), what specific variables mean (a codebook), what steps were taken to create specific maps (a research log), and how our data files are organized (a metadictionary). 

Our code can also be used as documentation if it is written using [literate programming](https://en.wikipedia.org/wiki/Literate_programming) techniques. In Stata, these techniques produce well annotated output that "weaves" together code, output, and narrative text that describes the function of the code and the results of the output. 

The third and final primary source of reproducibility in GISc is derived from our **organizational approach** to our work. GISc projects can require many gigabytes of data spread across dozens or even hundreds of files, feature classes, and databases. A disorgnaized file system can make replicating your work difficult if not impossible. Much of the research practices discussed in the remainder of this section are aimed at supporting one or more of these three major sources of reproducibility.

## Thinking in Workflows

One way to increase the reproducibility of a project is to approach each and every task with purposeful organization and thoughfulness. **Workflows** are the processes that we use to approach a given task. Think of checking your email. You (hopefully!) follow a series of steps when you check your email that help you organize your inbox. When I check my email for the first time each day, my workflow looks something like this:

  1. Delete junk mail
  2. Read and then delete New York Times and Washington Post morning newsletters
  3. Read and then delete SLU Newslink newsletter
  4. For each remaining email:
    a. Respond if response will take less than two minutes and/or
    b. forward to task management inbox if email requires an action, or
    c. snooze^[Snoozing is a "magical" feature of the email client that I use - [SparkMail](http://sparkmailapp.com).] the message until "later today" or "tomorrow morning" if response will require more time than currently avaiable.

In our reading for the first week of classes, [Scott Long](http://www.indiana.edu/~jslsoc/)^[Long, J.S., 2009. *The workflow of data analysis using Stata.* College Station, TX: Stata Press.] describes a structured strategy for approaching statistical research. In Long's model, a data analysis project consists of four steps: (a) data cleaning, (b) analysis, (c) presenting results, and (d) protecting files. This is a useful model to build upon for GISc work, and one that we will discuss over the course of the semester. 
  
Even more useful, not just for GISc work but for any process, are the tasks Long lays out for each step in the data analysis workflow:

  1. Planning
  2. Organization
  3. Documentation
  4. Execution

A good example of the utility of extending this logic to other workflows is with the problem sets. The "typical" approach students take with homework assignments is to sit down, open up their software, and start with question 1. Using Long's four task approach, a workflow-based strategy to the assignment would involve beginning by reading the assignment through in its entirety to develop a **plan** for approaching it - think about what techniques and skills are needed for each step. With a plan in place, you can proceed to **organizing** yourself for the assignment - identifying and obtaining files that you will need, creating dedicated directories for saving assignment data, and getting any necessary software documentation. After pulling together all of these materials, you are ready to move on to **documentation** - setting up your assignment code and output files, and (later in the course) your research log and meta-dictionary. Once you are set-up, you would then begin to address individual assignment questions as part of the **execution** task.

The goal here is to approach everything you do for research or work with an element of mindfullness and structure about your process. This mental model for approaching research supports the creation of **reproducible** research products because we approach our work in a routinized, predictable, organized, and efficient manner. Thinking in terms of workflows also encourages a greater awareness of the complexity of tasks, which also helps you plan more accurately for how long a particular task or project will take. 

In reality, there will be multiple workflows that you find yourself navigating. You will want a structured process not just for approaching a large spaital research project like the final project, but also a process for maintaining notes related to a specific assignment, a process for documenting code, a process for approaching assignments, and even a process for backing your data up. As you go through the course, think about how to best integrate these ideas into your work habits.

## Course Tools

This course relies on a number of major tools to help us accomplish the work that we need to do. This makes for a complex learning curve, particularly at the beginning of the semester. The tools we've selected for this class have been picked not necessarily because they are the *easiest* tools to learn, but because they *increase* our ability to conduct reproducible research. 

### ArcGIS
ArcGIS is the industry standard GIS application suite. Though there are other tools out there that contain much of the same functionaliy, ArcGIS remains the expected skillset for entry-level GIS jobs in nearly every sector of the labor market. ArcGIS excells at managing and manipulating spatial data, and has a wide range of tools for creating data visualizations that use spatial data. For these reasons, ArcGIS will occupy a large portion of our time this semester.

ArcGIS has two weaknesses for our purposes. While it can be scripted using [Python](https://www.python.org), that functionality is not placed front and center in the application. It also is difficult to pick up ArcGIS's Python tools, named [ArcPy](http://pro.arcgis.com/en/pro-app/arcpy/get-started/what-is-arcpy-.htm), without some background in GIS more generally. This limits the longterm reproducibility of the GIS work we'll do this semester since it is driven by the difficult to reproduce point-and-click user interface.

The second weakness that ArcGIS has is a limited set of tools for cleaning and manipulating tabular data. Not only are these driven by a point-and-click user interface, and thus limited in their ability to achieve reproducibility, but they are cumbersome and lack the power of other approaches to cleaning data.

### Stata
Since ArcGIS is limited in its approach to data cleaning, we will use [Stata](http://stata.com) instead. Stata is, first and foremost, statistical software. It has its own programming language and syntax that can be used not only for statistical purposes but also for cleaning data. For tabular data, therefore, Stata will become an intermediatory tool between raw data and data that is suitable for mapping. 

Our approach to using Stata will involve using a technique I described above called [literate programming](https://en.wikipedia.org/wiki/Literate_programming). The implementation of literate programming in Stata comes from a user-written package called **[Markdoc](http://haghish.com/statistics/stata-blog/reproducible-research/markdoc.php). Markdoc allows you to embed text that is formatted using the [markup language](https://en.wikipedia.org/wiki/Markup_language) [Markdown](https://daringfireball.net/projects/markdown/). Markdown is increasingly being adopted as one of the primary data science writing tools because it is (a) simple, (b) does not require extensive software or plugins, and (c) is widely support by applications like `R` and GitHub. We'll describe both Markdoc and Markdown further in the chapter "Reproducible Do-files".

### Atom
While Stata does have a built-in do-file editor, and you could easily use it to author code for Stata, writing in an external text editor has a number of advantages. Atom is a free, highly extensible, and easy to use text editor. Unlike Stata's do-file editor, it is not tied to a single application or programming language. And unlike Stata's editor, it cannot be readily extended, customized, or used for work outside of Stata. 

Atom, on the other hand, offers a large number of user-written packages that dramatically extend its base capabilities. One of those, `language-stata`, gives Atom support for working with Stata's do-file format. Atom also offers a text expansion tool that will help you write consistently structured and documented do-files. As we progress through the semester, we'll also use a number of packages for writing and previewing Markdown text files. So, while using Atom means adding an additional tool to your workflow, it also offers a number of improvements over what Stata comes built in with. We'll describe all of this further in the chapters on "Introducing Atom", "Introducing Markdown", and "Reproducible Do-files".

### GitHub
The final tool we'll use, GitHub and its desktop application GitHub Desktop, is an exceptionally powerful tool for conducting version control on an entire directory. This allows you to track changes in individual files as well as changes that impact an entire sub-folder or entire project directory. GitHub is increasingly recognized as one of the key tools available for making research reproducible because it allows users to maintain logs of every change they make on a project. It also offers other tools that support project management, including to-do lists, issue tracking, and even website maintaince. Since GitHub provides support for the [GeoJSON](http://geojson.org) standard for storing spatial data, we can store and preview(!) shapefiles on GitHub as well. GitHub, and the software that powers it called Git, are both described further in the "Introduction to GitHub".

## Course Workflow

One of the largest learning curves with this course is keeping track of how all of these tools fit together. This process is described in detail in [Week 2's lecture](https://github.com/slu-soc5650/week-02), but what follows is a short description of the "big picture" at work here. As we said above, it is important for you to being thinking in workflows. We've mapped out the major aspects of our course workflow to help aid that process:

```{r echo=FALSE, out.width = '100%'}
knitr::include_graphics("images/gisFlow1.png")
```

This workflow is premised on a common GISc situation: you have data stored in some type of database in a **tabular** or spreadsheet-like format that have a **spatial reference** like an address, which would allow them to be mapped. This main dataset of interest needs cleaning, as real world data often do, before it can be mapped. We'll use code written in Atom and executed by Stata to accomplish this task. We'll also use Atom to maintain and edit documentation that helps you increase the reproducibility of your work. 

Once the data are cleaned, we'll want to start working on mapping them. This cannot occur in a vaccum. Rather, you will need to seek out data sources that describe the physical or human geography in the area of interest. These may come as **shapefiles** or **geodatabases**, and they may also require some sort of data cleaning. Often the spatial data we have access to cover a larger area than what we need, or they cover too small an area and have to be merged with other files to capture the extent we require.

Once both our tabular and spatial data are cleaned, we can bring our tabular data in ArcGIS so that we can further clean it, if necessary, and map it. When we have maps ready for export, we often combine them into deliverables like presentations or printed booklets. This is best accomplished outside of ArcGIS in an application like PowerPoint or Word, or a more advanced publishing tool.

Finally, as we noted above, we will capture and track *almost* all of these files using GitHub:

```{r echo=FALSE, out.width = '100%'}
knitr::include_graphics("images/gisFlow2.png")
```

This workflow captures every aspect of a project from the bright idea that launches it to data acquisition, data cleaning, mapping, dissemination, and archiving. 
